---
title: "sparseWeightBasedPCA Package"
author: "Niek C. de Schipper <n.c.deschipper@uvt.nl>"
geometry: margin=1cm
date: "`r Sys.Date()`"
output:
    pdf_document:
        toc: true
    html_document:
        keep_md: true
    github_document:
         toc: true
    rmarkdown::html_vignette:
        toc: true
vignette: >
  %\VignetteIndexEntry{sparseWeightBasedPCA Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
library(sparseWeightBasedPCA)
```


# `sparseWeightBasedPCA`: A package for Regularized weight based Simultaneous Component Analysis (SCA) and Principal Component Analysis (PCA)

## Theoretical background

### Principal Component Analysis
Principal component analysis (PCA) is a widely used analysis technique for data reduction. It can give crucial insights in the underlying structure of the data when used as a latent variable model.

Given a data matrix $\mathbf{X}$ that contains the scores for $i = 1...I$ observations on $j = 1...J$ variables; we follow the convention to present the $J$ variable scores of observation $i$ in row $i$ and thus $\mathbf{X}$ has size $I \times J$. PCA decomposes the data into $Q$ components as follows,
\begin{equation}
    \label{PCA}
    \begin{aligned}
        &\mathbf{X} = \mathbf{XWP}^T + \mathbf{E} \\
        &\text{subject to } \mathbf{P}^T\mathbf{P} = \mathbf{I},
      \end{aligned}
\end{equation}
where $\mathbf{W}$ is a $J \times Q$ component weight matrix, $\mathbf{P}$ is a $J \times Q$ loading matrix and $\mathbf{E}$ is a $I \times J$ residual matrix. The component weight matrix $\mathbf{W}$ will be the focus of this package, note that $\mathbf{T} = \mathbf{XW}$ represent the component scores. 

The advantage of inspecting the component weights instead of the loadings is that you can directly derive meaning to $\mathbf{T}$, this because you see precisely in what way items in $\mathbf{X}$ are weighted together by $\mathbf{W}$.


### Simultaneous Component Analysis
The decomposition in (\ref{PCA}) can be extended to the case of multi-block data by taking $\mathbf{X}_c = [\mathbf{X}_{1} \hdots \mathbf{X}_{K}]$; this is concatenating the $K$ data blocks composed of different sets of variables of size $J_k$ for the same units of observation. The decomposition of $\mathbf{X}_c$ has the same block structured decomposition as in (\ref{PCA}) with $\mathbf{W}_c = [\mathbf{W}^{T}_{1} \hdots \mathbf{W}^{T}_{K}]^{T}$ and  $\mathbf{P}_c = [\mathbf{P}^{T}_{1} \hdots \mathbf{P}^{T}_{K}]^{T}$. This multi-block formulation of PCA is known as simultaneous component analysis (SCA):
\begin{equation}
    \label{SCA}
    \begin{aligned}
        &[\mathbf{X}_{1} \hdots \mathbf{X}_{K}] = 
            [\mathbf{X}_{1} \hdots \mathbf{X}_{K}]
            [\mathbf{W}^{T}_{1} \hdots \mathbf{W}^{T}_{K}]^{T}
            [\mathbf{P}^{T}_{1} \hdots \mathbf{P}^{T}_{K}] +  \mathbf{E} \\
        &\text{subject to } 
            [\mathbf{P}^{T}_{1} \hdots \mathbf{P}^{T}_{K}] 
            [\mathbf{P}^{T}_{1} \hdots \mathbf{P}^{T}_{K}]^{T} = \mathbf{I}
      \end{aligned}
\end{equation}
When analyzing multi-block data with SCA identifying meaningful relations between data blocks is of prime interest. In order to gain insight in what multiple data blocks relate to each other, we can search for blockwise structures in the component weights that tell us whether a component is uniquely determined by variables from one single data block (distinctive component), or whether it is a component that is determined by variables from multiple data blocks (common component). In other words, a distinctive component is a linear combination of variables of a particular data block only, whereas a common component is a linear combination of variables of multiple data blocks. An example of common and distinctive components in the situation with two data blocks is given below. The first two components are distinctive components, the third component is a common component, 
\[ \mathbf{T}
    =
    \begin{bmatrix}
        \mathbf{X}_1 & \mathbf{X}_2
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{W}_1 \\
        \mathbf{W}_2
    \end{bmatrix}
    =
    \begin{bmatrix}
        \mathbf{X}_1 & \mathbf{X}_2
    \end{bmatrix}
\begin{bmatrix}
    0 & w_{1_{1}2} & w_{1_{1}3}   \\
    0 & w_{2_{1}2} & w_{2_{1}3}   \\
    0 & w_{3_{1}2} & w_{3_{1}3}  
    \\[5pt]
    w_{1_{2}1} & 0 & w_{1_{2}3} \\
    w_{2_{2}1} & 0 & w_{2_{2}3} \\
    w_{3_{2}1} & 0 & w_{2_{2}3} 
\end{bmatrix}. \]

The `sparseWeightBasedPCA` package will provide functions that perform PCA and SCA on the component weights. It will also provide function for selection of the hyper parameters of the model. We will now describe the core models of this package that will be estimated by the following functions.

1. `scads` Regularized SCA with sparse component weights using constraints
2. `mmsca` Regularized SCA with sparse component weights using the group LASSO
3. `ccpca` PCA with sparse component weights using cardinality constraints

WRITE HERE WHAT OTHER PACKAGES DO

## Models of the `sparseWeightBasedPCA` package

### Regularized SCA with sparse component weights using constraints 

Here we present an approach of performing regularized SCA, with ridge and LASSO regularization and block wise constraints on $\mathbf{W}_c$ by solving,
\begin{equation}
\label{scads}
    \begin{aligned}
        L(\mathbf{W}_c, \mathbf{P}_c)=&\|\mathbf{X}_c -\mathbf{X}_c\mathbf{W}_c\mathbf{P}^T_{c}\|^{2}_{2} 
    + \lambda_L \| \mathbf{W}_c \|_{1} 
    + \lambda_R \| \mathbf{W}_c \|^{2}_{2} \\
        &\text{subject to } \mathbf{P}_c \mathbf{P}^{T}_c = \mathbf{I}, \text{ and } \lambda_L, \lambda_R \geq 0 \text{ and zero block constraints on } \mathbf{W}_c
    \end{aligned}
\end{equation}
In order to get a minimum for (\ref{scads}) we alternate between the estimation of  $\mathbf{W}_c$ and $\mathbf{P}_c$. Given $\mathbf{W}_c$ we can estimate $\mathbf{P}_c$ by using procrustes rotation. Given $\mathbf{P}_c$ we find estimates for $\mathbf{W}_c$ by using a coordinate descent algorithm that works by soft-thresholding the weights. For the specifics we refer the reader to (REF: NIEK KATRIJN). This iterative procedure stops when an optimum has been found (.i.e the loss function value is not decreasing anymore beyond pre-specified tolerance level). The optimization problem in (\ref{mmsca}) is non-convex and meaning there are local minima. In order to deal with that multiple random starts can be used with different initializations of $\mathbf{W}$, the start leading to the lowest evaluation of (\ref{mmsca}) is retained. Typically starting the algorithm with the solution of PCA (e.g. the first $Q$ right singular vectors of $\mathbf{X}$) will lead to smallest optimum.

The main advantage of analyzing multi-block data by using this procedure is that it is fast, and scalable to large data sets thanks to the coordinate descent implementation. The inclusion of the blockwise constraints on $\mathbf{W}$ make sure common and distinctive components are found and the LASSO and ridge regularizers are optional and facilitate extra sparsity within the component weights. A disadvantage of the method is that  the common and distinctive structures for $\mathbf{W}$ need to be selected. This has to be done using model selection procedures and can be computationally demanding.

This procedure has been implemented in the `scads` function. This function will be discussed in detail in the next section and examples will be given outlining the analysis including model selection.

### Regularized SCA with sparse component weights using the group LASSO

Here we present a very flexible approach of performing regularized SCA using, ridge, LASSO, group LASSO and elitist LASSO regularization by solving:
\begin{equation}
\label{mmsca}
    \begin{aligned}
        L(\mathbf{W}_c, \mathbf{P}_c)=&\|\mathbf{X}_c -\mathbf{X}_c\mathbf{W}_c\mathbf{P}^T_{c}\|^{2}_{2} 
    + \lambda_L \| \mathbf{W}_c \|_{1} 
    + \lambda_R \| \mathbf{W}_c \|^{2}_{2} \\
    &+ \sum_{q,k}(\lambda_{G} \sqrt{J_k}\| \mathbf{w}^{(k)}_{q} \|_{2}
    + \lambda_E \| \mathbf{w}^{(k)}_{q} \|_{1,2}) \\
        &\text{subject to } \mathbf{P}_c \mathbf{P}^{T}_c = \mathbf{I} \text{ and } \lambda_L, \lambda_R, \lambda_G, \lambda_E \geq 0
    \end{aligned}
\end{equation}
where $\mathbf{W}_c = [(\mathbf{W}^{(1)})^{T} \hdots (\mathbf{W}^{(K)})^{T}]^{T}$, and $\mathbf{w}^{(k)}_{q}$ denotes the $q$th column from the submatrix $\mathbf{W}^{(k)}$.
In order to get a minimum for (\ref{mmsca}) we alternate between the estimation of  $\mathbf{W}_c$ and $\mathbf{P}_c$. Given $\mathbf{W}_c$ we can estimate $\mathbf{P}_c$ by using procrustes rotation. Given $\mathbf{P}_c$ we can find estimates for $\mathbf{W}_c$ by using the majorization minimization (MM) algorithm. For the specifics we refer the reader to (REF: NIEK KATRIJN). This iterative procedure stops when an optimum has been found (.i.e the loss function value is not decreasing anymore beyond pre-specified tolerance level). The optimization problem in (\ref{mmsca}) is non-convex and meaning there are local minima. In order to deal with that multiple random starts can be used with different initializations of $\mathbf{W}$, the start leading to the lowest evaluation of (\ref{mmsca}) is retained. Typically starting the algorithm with the solution of PCA (e.g. the first $Q$ right singular vectors of $\mathbf{X}$) will lead to smallest optimum.

The main advantage of solving (\ref{mmsca}) is that it can automatically look for common and distinctive components by taking advantage of the properties of the group lasso. Because the group LASSO is specified on the colored segments (see below), it will either include these segments or put them zero, uncovering common and distinctive components. This is especially useful if the number of blocks and components is large, and an exhaustive approach of identifying common and distinctive is too computationally intensive.
\[ \mathbf{T}
    =
    \begin{bmatrix}
        \mathbf{X}_1 & \mathbf{X}_2
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{W}_1 \\
        \mathbf{W}_2
    \end{bmatrix}
    =
    \begin{bmatrix}
        \mathbf{X}_1 & \mathbf{X}_2
    \end{bmatrix}
\begin{bmatrix}
   \textcolor{blue}{w_{1_{1}1}}  & \textcolor{red}{w_{1_{1}2}} & \textcolor{orange}{w_{1_{1}3}}   \\
   \textcolor{blue}{w_{2_{1}1}}  & \textcolor{red}{w_{2_{1}2}} & \textcolor{orange}{w_{2_{1}3}}   \\
   \textcolor{blue}{w_{3_{1}1}}  & \textcolor{red}{w_{3_{1}2}} & \textcolor{orange}{w_{3_{1}3}}  
    \\[5pt]
    \textcolor{purple}{w_{1_{2}1}} &  \textcolor{green}{w_{1_{2}2}} & \textcolor{teal}{w_{1_{2}3}} \\
    \textcolor{purple}{w_{2_{2}1}} &  \textcolor{green}{w_{2_{2}2}} & \textcolor{teal}{w_{2_{2}3}} \\
    \textcolor{purple}{w_{3_{2}1}} &  \textcolor{green}{w_{3_{2}2}} & \textcolor{teal}{w_{2_{2}3}} 
\end{bmatrix}. \]
The inclusion the LASSO and ridge regularization are optional and facilitate extra sparsity within the colored segments. The elitist LASSO has a very special use case, the elitist LASSO will include all colored segments and will put weights within each segment to zero. The elitist lasso can be used to force components to be common. It is not advised to use the group LASSO and the elitist LASSO together as they have opposing goals. A disadvantage of using this procedure is that is potentially slow, this because its implemented using a MM-algorithm which tend to be slow in convergence. 

This procedure has been implemented in the `mmsca` function. This function will be discussed in detail in the next section and examples will be given outlining the analysis including model selection.

### PCA with sparse component weights using cardinality constraints

Here we present an approach of solving PCA by applying cardinality constraints to the component weights by solving:
\begin{equation}
\label{ccpca}
\begin{aligned}
        &L(\mathbf{W}, \mathbf{P})=\|\mathbf{X}-\mathbf{X}\mathbf{W}\mathbf{P}^T\|^{2}_{2} \\
        &\text{subject to } \mathbf{W} \text{ including } K \text{ zeros}.
\end{aligned}
\end{equation}
In order to get a minimum for (\ref{ccpca}) we need to alternate between the estimation of $\mathbf{W}$ and $\mathbf{P}$. Given $\mathbf{W}$ we can estimate $\mathbf{P}$ by using procruste rotation (REF: Ten_Berge_2005, Zou_2006), $\mathbf{P} = \mathbf{UV}^T$, where $\mathbf{U}$ and $\mathbf{V}$ are the left and right singular vectors of $\mathbf{X}^T\mathbf{XW}$. Given $\mathbf{P}$ we can find estimates for $\mathbf{W}$ given the cardinality constraints using the cardinality constraint regression algorithm for detail see (REF: NIEK KATRIJN). The optimization problem in (\ref{ccpca}) is non-convex and meaning there are local minima. In order to deal with that multiple random starts can be used with different initializations of $\mathbf{W}$, the start leading to the lowest evaluation of (\ref{ccpca}) is retained. Typically starting the algorithm with the solution of PCA (e.g. the first $Q$ right singular vectors of $\mathbf{X}$) will lead to smallest optimum.

The main advantage of solving (\ref{ccpca}) is that this model tries to directly tackle the problem of finding the underlying subset of weights, in contrast to the usage of a penalty that shrinks the weights and also induces sparsity such as the LASSO. This approach can lead to better discovery of the underlying weights compared to LASSO (REF: NIEK KATRIJN). Another advantage is that you directly impose cardinality constraints on $\mathbf{W}$. This gives the user total control over the amount of sparsity. This can be desirable if there is already an idea about the level of sparsity in the final model. 
 A disadvantage of using this procedure is that is potentially slow, this because the CCREG algorithm uses a MM-algorithm which tend to be slow in convergence. Another potential downside could be the absence of regularizers, they tend to shrink the variance of the estimators leading to more efficiency. In noisy situations other procedures might outperform this procedure.

This model has been implemented in the `ccpca` function. This function will be discussed in detail in the next section.


## The implementation in `R` of the `sparseWeightBasedPCA` package

The `sparseWeightBasedPCA` provides functions for the aforementioned models, also model selection procedures are provided in order to tune the hyper parameters. The main functions: `scads, mmsca` and `ccpca` are implemented in `C++` using the packages Rcpp (REF) and RcppArmadillo (REF). The model selection procedures are implemented in `R`.  The functions of the package will be discussed after which detailed examples will be given.


### Implementation of the main functions of the `sparseWeightBasedPCA` package

The main functions: `scads`, `mmsca` and `ccpca` are all implemented in a similar manor. At the core they perform SCA/PCA with variations depending on the goals of the user. An typical minimal use case `scads` given below (`mmsca` and `ccpca` are implemented very similarly) first generate some fake data:

```{r, eval = FALSE}
J <- 30
I <- 100
X <- matrix(rnorm(I*J), I, J)
```

With this data `scads` can be run with supplied values for the minimal required arguments. This minimal example performs PCA, with no constraints and, ridge and LASSO regularization. 

```{r, eval = FALSE}
ncomp <- 3 
constraints <- matrix(1, J, ncomp) # No constraints 
res <- scads(X, 
             ncomp = ncomp, 
             ridge = 10e-8, 
             lasso = rep(1, ncomp), 
             constraints = constraints, 
             Wstart = matrix(1, J, ncomp),
             itr = 10e5)  
```

All the main functions: `scads`, `mmsca` and `ccpca` output a list with the following named elements:

* `W` A matrix containing the component weights 
* `P` A matrix containing the loadings 
* `loss` A numeric variable containing the minimum loss function value of all the `nStarts` starts 
* `converged` A boolean containing `TRUE` if converged `FALSE` if not converged.

Details examples of data analysis will follow in the next section or see the documentation `?scads` et cetera. The main functions `scads`, `mmsca` and `ccpca` have a slightly different set of arguments. See the following list for the specifics.

#### Overview of the `scads` arguments

* `X` A data matrix of class `matrix`
* `ncomp` The number of components to estimate (an integer)
* `ridge` A numeric value containing the ridge parameter for ridge regularization on the component weight matrix W
* `lasso` A vector containing a ridge parameter for each column of W separately, to set the same lasso penalty for the component weights W, specify: lasso = `rep(value, ncomp)`
* `constraints` A matrix of the same dimensions as the component weights matrix W (`ncol(X)}` x `ncomp`). A zero entry corresponds in constraints corresponds to an element in the same location in W that needs to be constraint to zero. A non-zero entry corresponds to an element in the same location in W that needs to be estimated.
* `itr` The maximum number of iterations (an integer)
* `Wstart` A matrix of `ncomp` columns and `nrow(X)` rows with starting values for the component weight matrix W, if `Wstart` only contains zeros, a warm start is used: the first `ncomp` right singular vectors of X
* `tol` The convergence is determined by comparing the loss function value after each iteration, if the difference is smaller than tol, the analysis is converged. The default value is `10e-8`.
* `nStarts` The number of random starts the analysis should perform. The first start will be performed with the values given by `Wstart`. The consecutive starts will be `Wstart` plus a matrix with random uniform values times the current start number (the first start has index zero).
* `printLoss` A boolean: `TRUE` will print the loss function value each 10th iteration.


#### Overview of the `mmsca` arguments

* `X` A data matrix of class `matrix`
* `ncomp` The number of components to estimate (an integer)
* `ridge` A vector containing a ridge parameter for each column of W separately, to set the same ridge penalty for the component weights W, specify: ridge = `rep(value, ncomp)`, value is a non-negative double
* `lasso` A vector containing a ridge parameter for each column of W separately, to set the same lasso penalty for the component weights W, specify: lasso = `rep(value, ncomp)`, value is a non-negative double
* `grouplasso` A vector containing a grouplasso parameter for each column of W separately, to set the same grouplasso penalty for the component weights W, specify: grouplasso = `rep(value, ncomp)`, value is a non-negative double
* `elitistlasso` A vector containing a elitistlasso parameter for each column of W separately, to set the same elitistlasso penalty for the component weights W, specify: elitistlasso = `rep(value, ncomp)`, value is a non-negative double
* `groups` A vector specifying which columns of `X` belong to what block. Example: `c(10, 100, 1000)`. The first 10 variables belong to the first block, the 100 variables after that belong to the second block etc.
* `constraints` A matrix of the same dimensions as the component weights matrix W (`ncol(X)` x `ncomp`). A zero entry corresponds in constraints corresponds to an element in the same location in W that needs to be constraint to zero. A non-zero entry corresponds to an element in the same location in W that needs to be estimated.
* `itr` The maximum number of iterations (a positive integer)
* `Wstart` A matrix of `ncomp` columns and `nrow(X)` rows with starting values for the component weight matrix W, if `Wstart` only contains zeros, a warm start is used: the first `ncomp` right singular vectors of `X`
* `tol` The convergence is determined by comparing the loss function value after each iteration, if the difference is smaller than `tol`, the analysis is converged. Default value is `10e-8`
* `nStarts` The number of random starts the analysis should perform. The first start will be performed with the values given by `Wstart`. The consecutive starts will be `Wstart` plus a matrix with random uniform values times the current start number (the first start has index zero).
* `printLoss` A boolean: `TRUE` will print the loss function value each 10th iteration.
* `coorDes` A boolean with the default `FALSE`. If coorDes is `FALSE` the estimation of the majorizing function to estimate the component weights W conditional on the loadings P will be found using matrix inverses which can be slow. If set to true the majorizing function will be optimized (or partially optimized) using coordinate descent, in many cases coordinate descent will be faster
* `coorDesItr` An integer specifying the maximum number of iterations for the coordinate descent algorithm, the default is set to 1. You do not have to run this algorithm until convergence before alternating back to the estimation of the loadings. The tolerance for this algorithm is hard coded and set to `10^-8`. 


#### Overview of the `ccpca` arguments

* `X` A data matrix of class `matrix`
* `ncomp` The number of components to estimate (an integer)
* `nzeros` A vector of length `ncomp` containing the number of desired zeros in the columns of the component weight matrix `W`
* `itr` The maximum number of iterations (an integer)
* `Wstart` A matrix of `ncomp` columns and `nrow(X)` rows with starting values for the component weight matrix `W`, if `Wstart` only contains zeros, a warm start is used: the first `ncomp` right singular vectors of `X`
* `nStarts` The number of random starts the analysis should perform. The first start will be performed with the values given by `Wstart`. The consecutive starts will be `Wstart` plus a matrix with random uniform values times the current start number (the first start has index zero). The default value is 1.
* `tol` The convergence is determined by comparing the loss function value after each iteration, if the difference is smaller than `tol` the analysis is converged. The default value is `10e-8`
* `printLoss` A boolean: `TRUE` will print the loss function value each 10th iteration.


### Implementation of the model selection functions of the `sparseWeightBasedPCA` package

The to execute the `scads`, `mmsca` and `ccpca` arguments for the hyper parameters need to selected, for example the number of components or values for the `lasso` arguments. The `sparseWeightBasedPCA` package provides three procedures of selecting the hyper-parameters of the model: Cross-validation using the EigenVector method (REF), the Index of Sparseness (IS) and the Bayesian Information Criterion (BIC), for details see (REF). The procedures are implemented with in the functions: `CVforPCAwithSparseWeights`, `ISforPCAwithSparseWeights` and `BICforPCAwithSparseWeights`. They work as follows: first specify the arguments that the model selection function needs, then a pointer to the function that does the analysis should be given in the followed by its arguments. An example is given here:

```{r, eval = FALSE}
J <- 30
I <- 100
X <- matrix(rnorm(I*J), I, J)
```
With this data, `CVforPCAwithSparseWeights` using `scads` works as follows, 

```{r, eval = FALSE}
ncomp <- 3
res <- CVforPCAwithSparseWeights(X = X, 
                          nrFolds = 10, 
                          FUN = scads, # Pointer to scads(), followed by the arguments
                          ncomp = ncomp, 
                          ridge = 0, 
                          lasso = rep(0.1, ncomp),
                          constraints = matrix(1, J, ncomp),
                          Wstart = matrix(0, J, ncomp),
                          itr = 10e5,
                          printLoss = FALSE)
                          
```

This function returns a list with the following elements:

* `MSPE` The mean squared prediction error given the tuning parameters
* `MSPEstdError` The standard error of the `MSPE` 
* `nNonZeroCoef` The number of non-zero coefficients in the model 

Model selection of the hyper-parameters can be based on the model that results in the lowest mean squared prediction error, or a model can be picked with the least number of non-zero coefficients still within one standard error of the model with the lowest mean squared prediction error. The other functions work similar check see `?BICforPCAwithSparseWeights` and `?ISforPCAwithSparseWeights` for more detailed information.


### Additional tuning functions for `mmsca`

This package provides more elaborate model selection functions for `mmsca`, because of its flexibility it can be overwhelming to use the basic tuning function provided. Two additional functions are provided `mmscaModelSelection` and `mmscaHyperCubeSelection`. `mmscaModelSelection` uses a fixed grid of all combinations of the hyper-parameters to pick the best combination from, whereas `mmscaHyperCubeSelection` uses an adaptive grid that zooms in on a good combination of hyper-parameters until it converges on a certain combination. Note that `mmscaHyperCubeSelection` is experimental, it could potentially speed up the process of tuning enormously but it has not been scrutinized using a simulation study. A basic example of both is given here:

To perform model selection with `mmsca` using an exhaustive grid of all combinations of the tuning parameters the following can be done: 

```{r, eval = FALSE}
J <- 30
I <- 100
X <- matrix(rnorm(I*J), I, J)

out <- mmscaModelSelection(X, 
            ridgeSeq = seq(0, 1, by = 0.1), # Sequences of the hyper parameter
            lassoSeq = 0:100, 
            grouplassoSeq = 0, # No group lasso and no elitist lasso
            elitistlassoSeq = 0, 
            ncompSeq = 1:3, 
            tuningMethod = "CV", # Indicate the tuning method
            nrFolds = 10, 
            groups = ncol(X), # Arguments for mmsca()
            itr = 100000, 
            nStart = 1, 
            coorDes = FALSE, 
            coorDesItr = 100, 
            printProgress = TRUE)
```

This function returns a list with two elements: 

* `results` A list with `ncomp` elements each containing the following elements
    - `"BIC, IS or MSPE"` The index chosen in tuningMethod for all combinations of ridge, lasso, grouplasso and elististlasso
    - `"bestBIC, bestIS, bestMSPE or bestMSPE1stdErrorRule"` The best index according to the chosen tuning method
    - `"nNonZeroCoef"` The number of non zero weights in the best model
    - `"ridge"` The value of the ridge penalty corresponding to the best model
    - `"lasso"` The value of the lasso penalty corresponding to the best model
    - `"grouplasso"` The value of the group lasso penalty corresponding to the best model
    - `"elististlasso"` The value of the elitist lasso penalty corresponding to the best model
    - `"ncomp"` The number of component that was used for these items
    - `"ridge1stdErrorRule"` In case tuningMethod == "CV", the value of the ridge penalty according to the 1 standard error rule: the most sparse model within one standard error of the model with the lowest MSPE
    - `"lasso1stdErrorRule"` In case tuningMethod == "CV", the value of the lasso penalty according to the 1 standard error rule: the most sparse model within one standard error of the model with the lowest MSPE
    - `"grouplasso1stdErrorRule"` In case tuningMethod == "CV", the value of the group lasso penalty according to the 1 standard error rule: the most sparse model within one standard error of the model with the lowest MSPE
    - `"elitistlasso1stdErrorRule"` In case tuningMethod == "CV", the value of the elitist lasso penalty according to the 1 standard error rule: the most sparse model within one standard error of the model with the lowest MSPE
    - `"ridge1stdErrorRule"` In case tuningMethod == "CV", the value of the ridge according to the 1 standard error rule: the most sparse model within one standard error of the model with the lowest MSPE
*  `bestNcomp` The number of component with the best value for the chosen tuning index
 
For more details `?mmscaModelSelection`. This procedure can be slow because the number of combinations can be great it takes a lot of time to evaluate them all in order to pick the best one. To that end we also provide an alternative way of tuning using `mmscaHyperCubeSelection`. This function tunes a grid of the tuning parameters determine by the min and max of their corresponding sequences and a step size the provided by `stepsize` argument. It picks out the best combination, and zooms in on that combination, by making a new smaller grid around the previous best combination. This process continues until the average range of the sequences is less than 0.05. The new sequences are determined by taking the minimum value to be: best value - range, and maximum value by: best value + range, and a pre-specified step size in `stepsize`. In order for this procedure to work well, the grid needs to include an optimal combination of tuning parameters, and it needs a reasonable step size (at least 3, 5 is better, 2 is too small). This approach assumes that a local optimum of tuning parameters is good enough to get interpretable results. Note that this function is experimental and has not been scrutinized using a simulation study. This procedure can be performed as follows,

```{r, eval = FALSE}

out <- mmscaHyperCubeSelection(X,
              ncomp = 3, # This function works with a fixed number of components
              ridgeSeq = 0:3,
              lassoSeq = 0:10,
              grouplassoSeq = 0,
              elitistlassoSeq = 0,
              stepsize = 5, # Step size of the sequences
              logscale = FALSE, # Sequences can be on the log-scale
              method = "CV1stdError", # Tuning method
              groups = ncol(X), # Arguments for mmsca()
              nStart = 1,
              itr = 100000,
              printProgress = TRUE,
              coorDes = FALSE,
              coorDesItr = 1,
              tol = 10e-5,
              nrFolds = 10)
```

This function returns a list with the following elements:

* `ridge` A vector with `ncomp` elements all equal to the chosen ridge value 
* `lasso` A vector with `ncomp` elements all equal to the chosen lasso value 
* `grouplasso` A vector with `ncomp` elements all equal to the chosen group lasso value 
* `elitistlasso` A vector with `ncomp` elements all equal to the chosen elitist lasso value 

For more details see `?mmscaHyperCubeSelection`. 


## Detailed examples of SCA and PCA with the `sparseWeightBasedPCA` package

In this section we will give some detailed examples where we analyze multi- and single block data with SCA and PCA including the model selection process. We given an examples of `scads`, `mmsca` and `ccpca` and the model selection functions. We will demonstrate these procedures using simulated data, for that purpose the package includes data generating functions that simulates data according to the following structure,
\begin{equation}
    \mathbf{X} = \mathbf{X}\mathbf{W}\mathbf{P}^T,
\end{equation}
where $\mathbf{W}$ is $J\times J$, $\mathbf{W}^T\mathbf{W} = \mathbf{I}$ and $\mathbf{W} = \mathbf{P}$. $\mathbf{W}$ is manipulated such that it contains a given level of sparsity in the first $Q$ columns. The covariance matrix of $\mathbf{X}$ can be constructed by taking $\mathbf{\Sigma} = \mathbf{W}\mathbf{\Lambda}\mathbf{W}^T$, the eigenvalues in $\mathbf{\Lambda}$ can be manipulated to control the variance of the signal components versus the variance of the noise components. Using the covariance matrix data can be sampled from the multivariate normal distribution using `mvrnorm` from the package MASS (REF). Functions for data generation are provided by: `sparsify` to put sparsity in $\mathbf{W}$, `makeVariance` to manipulate the eigenvalues in $\mathbf{\Lambda}$, and `makeDat` to simulate the data. Check the package documentation for more details.


### Example of SCA with `scads`

Here we will demonstrate data analysis using `scads`. In this example we will have 2 data blocks each with 15 variables and 3 components. First we create a common and distinctive structure for the component weights to generate data from, we will use a structure with 2 distinctive components and 1 common component. 

```{r}

set.seed(1)
ncomp <- 3 
J <- 30
comdis <- matrix(1, J, ncomp) # Component weight structure
comdis[1:15, 1] <- 0 # The first component is distinctive for the first block
comdis[16:30, 2] <- 0 # The second component is distinctive for the second block
comdis <- sparsify(comdis, 0.2) #set 20 percent of the 1's to zero
comdis

```

Now given this component weight structure we can simulate data as follows,

```{r}

variances <- makeVariance(varianceOfComps = c(100, 80, 90), 
                          J = J, error = 0.05) #create realistic eigenvalues
plot(variances, xlab ="components", 
     ylab ="Eigenvalues", main = "Scree plot of the Eigenvalues X")
dat <- makeDat(n = 100, comdis = comdis, variances = variances)
X <- dat$X
round(dat$P[, 1:ncomp], 3) # The data generating component weight structure

```

In the scree plot you can see the eigenvalues of the data generating covariance matrix and `round(dat$P[, 1:ncomp], 3)` prints the data generating component weights for the signal components. Given this generate data set we can perform `scads` by first looking for the common and distinctive structure by trying all structures out and finding the best structure according to model selection, in this case we will use cross-validation. To generate all common and distinctive structures we implemented a function called `allCommonDistinctive` for more details see the documentation.

```{r, eval = FALSE}
# Generate all possible common and distinctive structures
allstructures <- allCommonDistinctive(vars = c(15, 15), ncomp = 3, allPermutations = TRUE, filterZeroSegments = TRUE)

#Use cross-validation to look for the data generating structure 
index <- rep(NA, length(allstructures))
for (i in 1:length(allstructures)) {
    index[i] <- CVforPCAwithSparseWeights(X = X, nrFolds = 10, FUN = scads, 
                          ncomp, ridge = 0, lasso = rep(0, ncomp),
                          constraints = allstructures[[i]], Wstart = matrix(0, J, ncomp),
                          itr = 100000, nStarts = 1, printLoss = FALSE, tol = 10^-5)$MSPE
}

winningStructure <- allstructures[[which.min(index)]] 
allstructures[[which.min(index)]] # print the best common and distinctive structure

```

Given this common and distinctive structure we can tune the `lasso` parameter to get some sparsity inside the weights, for this we will use cross validation with the one standard error rule. This entails cross validating the model a bunch of times for different values of the `lasso` parameter, from which we will select the model with the most weights at zero still within one standard error of the model with the least mean squared prediction error (MSPE). If there are not models within one standard error of the best model you should decrease the step size of the lasso sequence.

```{r, eval = FALSE}

lasso <- exp(seq(log(0.0000001), log(1), length.out = 100)) # Generate candidate lasso values
MSPE <- rep(NA, length(lasso))
MSPEstdError <- rep(NA, length(lasso))
nNonZeroCoef <- rep(NA, length(lasso))

for (i in 1:length(lasso)) {
    res <- CVforPCAwithSparseWeights(X = X, nrFolds = 10, FUN = scads, 
                          ncomp, ridge = 0, lasso = rep(lasso[i], ncomp),
                          constraints = winningStructure, Wstart = matrix(0, J, ncomp),
                          itr = 100000, nStarts = 1, printLoss = FALSE, tol = 10^-5)
    MSPE[i] <- res$MSPE # Store MSPE for each lasso value
    MSPEstdError[i] <- res$MSPEstdError # Store the standard error of the MSPE
    nNonZeroCoef[i] <- res$nNonZeroCoef # Store the number of non-zero weights
}

x <- 1:length(lasso)
plot(x , MSPE, xlab = "lasso", ylab = "MSPE",
     main = "MSPE with one standard error for different lasso values")

# Add error bars to the plot
arrows(x, MSPE - MSPEstdError, x , MSPE + MSPEstdError, length = 0.05, angle = 90, code = 3)

# Select all models within one standard error of the best model
eligibleModels <- MSPE < MSPE[which.min(MSPE)] + MSPEstdError[which.min(MSPE)]

# Selected from those models the models with the lowest number of non-zero weights
best <- which.min(nNonZeroCoef[eligibleModels])

# Do the analysis with the "winning" structure and best lasso
results <- scads(X = X, ncomp = ncomp, 
                ridge = 0, lasso = rep(lasso[best], ncomp),
                constraints = allstructures[[which.min(index)]], 
                Wstart = matrix(0, J, ncomp),
                itr = 100000, nStarts = 1, printLoss = FALSE , tol = 10^-5)

# Compare results from the analysis with the data generating model
compare <- cbind(dat$P[, 1:ncomp], results$W)
colnames(compare) <- c(paste("True W_", 1:3, sep = ""), paste("Est W_", 1:3, sep = ""))
rownames(compare) <- paste("Var", 1:30) 
round(compare, 3)

```

This concludes the analysis of multi-block data with `scads`. This procedure offers lots of flexibility to the user at the cost of a little more complexity (i.e. the user has to know for-loops and some `R` basics).


### Example of SCA with `mmsca`


```{r}


results <- mmscaHyperCubeSelection(X,
              ncomp = 3,
              ridgeSeq = 0,
              lassoSeq = 0:1,
              grouplassoSeq = 0:1,
              elitistlassoSeq = 0,
              stepsize = 5,
              logscale = TRUE,
              groups = ncol(X),
              nStart = 1,
              itr = 100000,
              printProgress = TRUE,
              coorDes = FALSE,
              coorDesItr = 1,
              method = "CV1stdError",
              tol = 10e-5,
              nrFolds = 10)

fit <- mmsca(X = X, 
    ncomp = ncomp, 
    ridge = rep(0, ncomp),
    lasso = results$lasso,
    grouplasso = results$grouplasso,
    elitistlasso = rep(0, ncomp),
    groups = c(J/2, J/2), 
    constraints = matrix(1, J, ncomp), 
    itr = 1000000, 
    Wstart = matrix(0, J, ncomp),
    nStarts = 1)


fit <- mmsca(X = X, 
    ncomp = ncomp, 
    ridge = rep(0, ncomp),
    lasso = rep(0.0001, ncomp),
    grouplasso = rep(0.0001, ncomp),
    elitistlasso = rep(0, ncomp),
    groups = c(J/2, J/2), 
    constraints = matrix(1, J, ncomp), 
    itr = 1000000, 
    Wstart = matrix(0, J, ncomp),
    nStarts = 1)
?mmsca

fit$W
dat$P[, 1:ncomp]
lasso

lasso <- exp(seq(log(0.001), log(1), length.out = 10)) # Generate candidate lasso values
grouplasso <- exp(seq(log(0.001), log(1), length.out = 10)) # Generate candidate lasso values

mmscaModelSelection(X, 
        ridgeSeq = 0,
        lassoSeq = lasso,
        grouplassoSeq = grouplasso,
        elitistlassoSeq = 0, 
        ncompSeq = 3, 
        tuningMethod = "CV", 
        groups = c(15, 15), 
        nrFolds = 10, 
        itr = 10000, 
        nStart = 1, 
        tol = 10e-5,
        coorDes = FALSE, 
        coorDesItr = 100, 
        printProgress = TRUE)


```












